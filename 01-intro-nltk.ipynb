{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692845f-b0ea-4a1f-accf-4c7a00f5b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading all of the nltk packages\n",
    "import os\n",
    "import pathlib\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "# Use the notebook's current working directory\n",
    "NOTEBOOK_DIR = pathlib.Path().resolve()\n",
    "NLTK_DIR = NOTEBOOK_DIR / \"nltk_data\"\n",
    "NLTK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Make NLTK look here first\n",
    "nltk.data.path.insert(0, str(NLTK_DIR))\n",
    "\n",
    "nltk.download('popular',    download_dir=NLTK_DIR)\n",
    "nltk.download('punkt',      download_dir=NLTK_DIR)  # tokenizer models\n",
    "nltk.download('punkt_tab',  download_dir=NLTK_DIR)  # tokenizer lookup tables (needed in newer NLTK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af819f9-14a2-409d-9037-6e3e2b96b35c",
   "metadata": {},
   "source": [
    "## Common nltk functions\n",
    "\n",
    "### nltk.sent_tokenize\n",
    "splits a text into individual sentences using trained punctuation and language rules (Punkt tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7162ff-9f4b-4a05-94e4-ba8dd64d9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Backgammon is one of the oldest known board games.\",\n",
      "  \"Its history goes back almost 5,000 years to archaeological discoveries in the Middle East.\",\n",
      "  \"It is a two-player game in which each player has fifteen checkers that move between twenty-four points according to the roll of two dice.\"\n",
      "]\n",
      "Number of sentences: 3\n"
     ]
    }
   ],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history goes back almost 5,000 years to archaeological discoveries in the Middle East. It is a two-player game in which each player has fifteen checkers that move between twenty-four points according to the roll of two dice.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(json.dumps(sentences, indent=2, ensure_ascii=False))\n",
    "print(f\"Number of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d40572-a02f-4a46-97a4-835915d7588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Backgammon is one of the oldest known board games.\",\n",
      "  \"Its history goes back almost 5,000 years to archaeological discoveries in the Middle East.\",\n",
      "  \"It is a two-player game in which each player has fifteen checkers that move between twenty-four points according to the roll of two dice.\"\n",
      "]\n",
      "Number of sentences: 3\n"
     ]
    }
   ],
   "source": [
    "text_line_list = (\"Backgammon is one of the oldest known board games. \"\n",
    "        \"Its history goes back almost 5,000 years to archaeological discoveries in the Middle East. \"\n",
    "        \"It is a two-player game in which each player has fifteen checkers that move between twenty-four points \"\n",
    "        \"according to the roll of two dice.\")\n",
    "\n",
    "sentences_line_list = nltk.sent_tokenize(text_line_list)\n",
    "\n",
    "print(json.dumps(sentences_line_list, indent=2, ensure_ascii=False))\n",
    "print(f\"Number of sentences: {len(sentences_line_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26487505-970d-416a-85ed-21896ae6ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Backgammon\", \"is\", \"one\", \"of\", \"the\", \"oldest\", \"known\", \"board\", \"games\", \".\", \"Its\", \"history\", \"goes\", \"back\", \"almost\", \"5,000\", \"years\", \"to\", \"archaeological\", \"discoveries\", \"in\", \"the\", \"Middle\", \"East\", \".\", \"It\", \"is\", \"a\", \"two-player\", \"game\", \"in\", \"which\", \"each\", \"player\", \"has\", \"fifteen\", \"checkers\", \"that\", \"move\", \"between\", \"twenty-four\", \"points\", \"according\", \"to\", \"the\", \"roll\", \"of\", \"two\", \"dice\", \".\"]\n",
      "Number of words: 50\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "print(json.dumps(words, ensure_ascii=False))\n",
    "print(f\"Number of words: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd7b1b-f7b9-4ef9-8261-8fd24c53de81",
   "metadata": {},
   "source": [
    "- Stemmer: A tool that chops words down to their base form by heuristic rules (e.g., “running” → “run”, “studies” → “studi”). Fast, crude, may produce non-words.\n",
    "- Lemmatizer: A tool that reduces words to their dictionary form (lemma) using vocabulary and grammar (e.g., “better” → “good”, “running” → “run”). Slower, more accurate, returns real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d64ddbca-e000-4248-a1e7-105f1fcb3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    # Print the results of the stem and lemmatize, using word and pos (Part of Speech)\n",
    "    print(f\"Stemmer: {stemmer.stem(word)}\")\n",
    "    print(f\"Lemmatizer: {lemmatizer.lemmatize(word,pos)}\")\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "compare_stemmer_and_lemmatizer(stemmer,lemmatizer,word='seen',pos=wordnet.VERB)\n",
    "print()\n",
    "compare_stemmer_and_lemmatizer(stemmer,lemmatizer,word='drove',pos=wordnet.VERB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0702ca-2854-45ff-85ff-a0897c0aaa22",
   "metadata": {},
   "source": [
    "## Stop words are words that are filtered out before or after processing the text.\n",
    "\n",
    "When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
    "\n",
    "Stop words generally refer to the most common words like “and,” “the,” “a” in a language, but there is no single universal stop‑word list. The list of stop words can change depending on your application.\n",
    "\n",
    "The NLTK toolkit has a predefined list of keywords that refers to the most common words. If you are using it for the first time, you need to download the stop words with this code: nltk.download(\"stopwords\"). Once the download is complete, you can load the stop‑words package from nltk.corpus and use it to load the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a36ed9e-75f3-445b-9f88-67fcc3b61de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3de231bb-36c0-44f4-84f0-f92dc6edc2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.', 'Its', 'history', 'goes', 'back', 'almost', '5,000', 'years', 'archaeological', 'discoveries', 'Middle', 'East', '.', 'It', 'two-player', 'game', 'player', 'fifteen', 'checkers', 'move', 'twenty-four', 'points', 'according', 'roll', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621c48f-22a0-4a3a-8344-da70b6071f19",
   "metadata": {},
   "source": [
    "However, keep in mind that list comprehensions are faster because they’re optimized so that Python’s interpreter can detect a predictable pattern during the loop.\n",
    "\n",
    "You might wonder why we convert our list into a set. A set is an abstract data type that can store unique values with no particular order. Lookups in a set are much faster than lookups in a list. For a small number of words there’s not much difference, but if you have a large number of words, it’s strongly recommended to use a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa87bb-c429-4069-9b13-b3e5380493d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
